{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('b2w-10k.csv')\n",
    "df3 = df[pd.to_numeric(df['overall_rating'], errors='coerce').between(0, 5, inclusive='both')]\n",
    "df3 = df3[['review_text', 'overall_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino vectorizado: (7499, 50, 100)\n",
      "Tamanho do conjunto de teste vectorizado: (2500, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df3['review_text'],\n",
    "                                                    df3['overall_rating'],\n",
    "                                                    test_size=0.25, random_state=42)\n",
    "\n",
    "# Tokenizar os textos\n",
    "X_train_tokenized = [text.split() for text in X_train]\n",
    "X_test_tokenized = [text.split() for text in X_test]\n",
    "\n",
    "# Passo 2: Treinar o modelo Word2Vec no conjunto de treino\n",
    "w2v_model = gensim.models.Word2Vec(sentences=X_train_tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Passo 3: Criar função para transformar o texto em vetores utilizando o modelo Word2Vec\n",
    "def text_to_vector(text, model, max_len):\n",
    "    vectorized_text = []\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            vectorized_text.append(model.wv[word])\n",
    "        else:\n",
    "            vectorized_text.append(np.zeros(model.vector_size))  # Se a palavra não estiver no vocabulário\n",
    "    # Padding/truncamento para garantir que o comprimento seja fixo (max_len)\n",
    "    if len(vectorized_text) < max_len:\n",
    "        vectorized_text.extend([np.zeros(model.vector_size)] * (max_len - len(vectorized_text)))\n",
    "    else:\n",
    "        vectorized_text = vectorized_text[:max_len]\n",
    "    return np.array(vectorized_text)\n",
    "\n",
    "# Passo 4: Aplicar a transformação para todo o conjunto de treino e teste\n",
    "max_len = 50  # Definido anteriormente\n",
    "\n",
    "X_train_vectorized = np.array([text_to_vector(text, w2v_model, max_len) for text in X_train_tokenized])\n",
    "X_test_vectorized = np.array([text_to_vector(text, w2v_model, max_len) for text in X_test_tokenized])\n",
    "\n",
    "# Verificar os shapes resultantes\n",
    "print(f\"Tamanho do conjunto de treino vectorizado: {X_train_vectorized.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste vectorizado: {X_test_vectorized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uni-direcional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hub/COURSERA/NLP/nlp/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "max_features = 15000  # Número máximo de palavras no vocabulário\n",
    "max_len = 50  # Comprimento máximo da sequência (ajustado com base nos dados)\n",
    "\n",
    "model_lstm_uni = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=100, input_length=max_len),\n",
    "    LSTM(64),  # Unidirecional\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm_uni.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-direcional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model_lstm_bi = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=100, input_length=max_len),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm_bi.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uni-direcional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model_gru_uni = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=100, input_length=max_len),\n",
    "    GRU(64),  # Unidirecional\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_gru_uni.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-direcional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru_bi = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=100, input_length=max_len),\n",
    "    Bidirectional(GRU(64)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_gru_bi.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hub/COURSERA/NLP/nlp/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 - 6s - 26ms/step - accuracy: 0.1964 - loss: -3.6156e+01 - val_accuracy: 0.1780 - val_loss: -6.2386e+01\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -8.0972e+01 - val_accuracy: 0.1780 - val_loss: -1.0453e+02\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -1.2084e+02 - val_accuracy: 0.1780 - val_loss: -1.4507e+02\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -1.5979e+02 - val_accuracy: 0.1780 - val_loss: -1.8498e+02\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -1.9838e+02 - val_accuracy: 0.1780 - val_loss: -2.2466e+02\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -2.3672e+02 - val_accuracy: 0.1780 - val_loss: -2.6418e+02\n",
      "Epoch 7/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -2.7501e+02 - val_accuracy: 0.1780 - val_loss: -3.0449e+02\n",
      "Epoch 8/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -3.1476e+02 - val_accuracy: 0.1780 - val_loss: -3.4538e+02\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 12ms/step - accuracy: 0.1964 - loss: -3.5399e+02 - val_accuracy: 0.1780 - val_loss: -3.8562e+02\n",
      "Epoch 10/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -3.9291e+02 - val_accuracy: 0.1780 - val_loss: -4.2567e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hub/COURSERA/NLP/nlp/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 - 6s - 24ms/step - accuracy: 0.1958 - loss: -6.9422e+01 - val_accuracy: 0.1780 - val_loss: -1.1882e+02\n",
      "Epoch 2/10\n",
      "235/235 - 4s - 15ms/step - accuracy: 0.1964 - loss: -1.5458e+02 - val_accuracy: 0.1780 - val_loss: -2.0047e+02\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 15ms/step - accuracy: 0.1964 - loss: -2.3261e+02 - val_accuracy: 0.1780 - val_loss: -2.8003e+02\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -3.0922e+02 - val_accuracy: 0.1780 - val_loss: -3.5875e+02\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -3.8534e+02 - val_accuracy: 0.1780 - val_loss: -4.3712e+02\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 15ms/step - accuracy: 0.1964 - loss: -4.6113e+02 - val_accuracy: 0.1780 - val_loss: -5.1524e+02\n",
      "Epoch 7/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -5.3678e+02 - val_accuracy: 0.1780 - val_loss: -5.9326e+02\n",
      "Epoch 8/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -6.1233e+02 - val_accuracy: 0.1780 - val_loss: -6.7116e+02\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -6.8775e+02 - val_accuracy: 0.1780 - val_loss: -7.4891e+02\n",
      "Epoch 10/10\n",
      "235/235 - 4s - 15ms/step - accuracy: 0.1964 - loss: -7.6307e+02 - val_accuracy: 0.1780 - val_loss: -8.2684e+02\n",
      "Epoch 1/10\n",
      "235/235 - 4s - 18ms/step - accuracy: 0.1963 - loss: -3.9867e+01 - val_accuracy: 0.1780 - val_loss: -6.7421e+01\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -8.5604e+01 - val_accuracy: 0.1780 - val_loss: -1.0921e+02\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -1.2545e+02 - val_accuracy: 0.1780 - val_loss: -1.4997e+02\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -1.6471e+02 - val_accuracy: 0.1780 - val_loss: -1.9028e+02\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 11ms/step - accuracy: 0.1964 - loss: -2.0370e+02 - val_accuracy: 0.1780 - val_loss: -2.3047e+02\n",
      "Epoch 6/10\n",
      "235/235 - 5s - 20ms/step - accuracy: 0.1964 - loss: -2.4255e+02 - val_accuracy: 0.1780 - val_loss: -2.7042e+02\n",
      "Epoch 7/10\n",
      "235/235 - 5s - 23ms/step - accuracy: 0.1964 - loss: -2.8127e+02 - val_accuracy: 0.1780 - val_loss: -3.1046e+02\n",
      "Epoch 8/10\n",
      "235/235 - 5s - 23ms/step - accuracy: 0.1964 - loss: -3.1994e+02 - val_accuracy: 0.1780 - val_loss: -3.5024e+02\n",
      "Epoch 9/10\n",
      "235/235 - 5s - 22ms/step - accuracy: 0.1964 - loss: -3.5860e+02 - val_accuracy: 0.1780 - val_loss: -3.9020e+02\n",
      "Epoch 10/10\n",
      "235/235 - 5s - 22ms/step - accuracy: 0.1964 - loss: -3.9718e+02 - val_accuracy: 0.1780 - val_loss: -4.2998e+02\n",
      "Epoch 1/10\n",
      "235/235 - 7s - 30ms/step - accuracy: 0.1960 - loss: -7.0573e+01 - val_accuracy: 0.1780 - val_loss: -1.2037e+02\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -1.5625e+02 - val_accuracy: 0.1780 - val_loss: -2.0229e+02\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -2.3468e+02 - val_accuracy: 0.1780 - val_loss: -2.8247e+02\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -3.1216e+02 - val_accuracy: 0.1780 - val_loss: -3.6231e+02\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 15ms/step - accuracy: 0.1964 - loss: -3.8929e+02 - val_accuracy: 0.1780 - val_loss: -4.4184e+02\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 13ms/step - accuracy: 0.1964 - loss: -4.6617e+02 - val_accuracy: 0.1780 - val_loss: -5.2087e+02\n",
      "Epoch 7/10\n",
      "235/235 - 3s - 15ms/step - accuracy: 0.1964 - loss: -5.4290e+02 - val_accuracy: 0.1780 - val_loss: -6.0025e+02\n",
      "Epoch 8/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -6.1952e+02 - val_accuracy: 0.1780 - val_loss: -6.7918e+02\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -6.9606e+02 - val_accuracy: 0.1780 - val_loss: -7.5822e+02\n",
      "Epoch 10/10\n",
      "235/235 - 3s - 14ms/step - accuracy: 0.1964 - loss: -7.7254e+02 - val_accuracy: 0.1780 - val_loss: -8.3712e+02\n"
     ]
    }
   ],
   "source": [
    "# Função para treinar e avaliar os modelos\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, epochs=5, batch_size=32):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return test_acc\n",
    "\n",
    "# Definir os modelos sem a camada de Embedding\n",
    "def create_lstm_uni_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, input_shape=(max_len, 100)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_lstm_bi_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), input_shape=(max_len, 100)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_gru_uni_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.GRU(64, input_shape=(max_len, 100)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_gru_bi_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64), input_shape=(max_len, 100)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Treinando os quatro modelos\n",
    "results = {}\n",
    "\n",
    "# LSTM Unidirecional\n",
    "model_lstm_uni = create_lstm_uni_model()\n",
    "accuracy_lstm_uni = train_and_evaluate_model(model_lstm_uni, X_train_vectorized, y_train, X_test_vectorized, y_test, epochs=10)\n",
    "results['LSTM uni-direcional'] = accuracy_lstm_uni\n",
    "\n",
    "# LSTM Bidirecional\n",
    "model_lstm_bi = create_lstm_bi_model()\n",
    "accuracy_lstm_bi = train_and_evaluate_model(model_lstm_bi, X_train_vectorized, y_train, X_test_vectorized, y_test, epochs=10)\n",
    "results['LSTM bi-direcional'] = accuracy_lstm_bi\n",
    "\n",
    "# GRU Unidirecional\n",
    "model_gru_uni = create_gru_uni_model()\n",
    "accuracy_gru_uni = train_and_evaluate_model(model_gru_uni, X_train_vectorized, y_train, X_test_vectorized, y_test, epochs=10)\n",
    "results['GRU uni-direcional'] = accuracy_gru_uni\n",
    "\n",
    "# GRU Bidirecional\n",
    "model_gru_bi = create_gru_bi_model()\n",
    "accuracy_gru_bi = train_and_evaluate_model(model_gru_bi, X_train_vectorized, y_train, X_test_vectorized, y_test, epochs=10)\n",
    "results['GRU bi-direcional'] = accuracy_gru_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Método  Acurácia Melhor (S/N)\n",
      "0  LSTM uni-direcional     0.178            S\n",
      "1   LSTM bi-direcional     0.178            N\n",
      "2   GRU uni-direcional     0.178            N\n",
      "3    GRU bi-direcional     0.178            N\n"
     ]
    }
   ],
   "source": [
    "# Identificar o melhor modelo\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Criar uma lista de dicionários com os dados dos modelos\n",
    "data = []\n",
    "for model_name, accuracy in results.items():\n",
    "    is_best = \"S\" if model_name == best_model else \"N\"\n",
    "    data.append({\"Método\": model_name, \"Acurácia\": accuracy, \"Melhor (S/N)\": is_best})\n",
    "\n",
    "# Criar um DataFrame com os resultados\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "# Exibir o DataFrame com os resultados\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
